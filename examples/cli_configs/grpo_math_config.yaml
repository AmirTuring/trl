# GRPO Math Training Configuration
# Example configuration for training mathematical reasoning with GRPO

# Dataset configuration
datasets:
  - path: AmirMohseni/Big-Math-RL-filtered
    split: train

# Field mapping configuration
# Maps dataset fields to expected question/answer format
field_mapping:
  question_field: question
  answer_field: answer_correct

# Model configuration  
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
model_revision: main
torch_dtype: bfloat16
#attn_implementation: flash_attention_2
bf16: true

# Training configuration
output_dir: runs/qwen-2.5-1.5b
num_train_epochs: 1
max_steps: 1000
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
learning_rate: 5.0e-7
lr_scheduler_type: cosine
warmup_ratio: 0.03

# GRPO specific parameters
beta: 0.001
max_prompt_length: 512
max_completion_length: 2048
num_generations: 16

# VLLM configuration for fast generation
use_vllm: true
vllm_mode: colocate
vllm_gpu_memory_utilization: 0.5

# Logging configuration
logging_strategy: steps
logging_steps: 10
save_strategy: steps
save_steps: 100
save_total_limit: 2

# Script-specific arguments
dataset_seed: 42

# PEFT configuration
use_peft: false

# Reporting
report_to: 
  - wandb
run_name: grpo-qwen-2.5-1.5b-22-10

# Hugging Face Hub
push_to_hub: true
hub_strategy: every_save
hub_model_id: AmirMohseni/grpo-qwen-2.5-1.5b
