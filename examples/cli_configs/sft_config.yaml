# SFT Training Configuration Example
# This configuration demonstrates the enhanced SFT script features

# Model configuration
model_name_or_path: "Qwen/Qwen2-0.5B"  
trust_remote_code: true
attn_implementation: "flash_attention_2"

# Dataset configuration
datasets:
  - path: "trl-lib/Capybara"
    split: "train"

# Optional validation dataset configuration
# If provided, will be used for evaluation during training
validation_datasets:
  - path: "trl-lib/Capybara"
    split: "test"
    
# Training arguments
learning_rate: 2.0e-5
num_train_epochs: 1
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
gradient_checkpointing: true
packing: true
max_seq_length: 2048

# Evaluation
eval_strategy: "steps"
eval_steps: 100
per_device_eval_batch_size: 4

# Logging and tracking
logging_steps: 10
output_dir: "./sft-qwen2-0.5b"
run_name: "sft-qwen2-capybara"
project_name: "sft-training"
report_to: ["wandb"]

# LoRA configuration (optional)
use_peft: true
lora_r: 32
lora_alpha: 16
lora_dropout: 0.1
lora_target_modules: "all-linear"

# Saving and pushing
save_strategy: "steps"
save_steps: 200
push_to_hub: false  # Set to true to push to HuggingFace Hub

# Field mapping configuration
# Maps dataset fields to expected question/completion format
field_mapping:
  question_field: "question"
  completion_field: "completion"  # or "response"

# Optional validation field mapping configuration
# If not provided, will use the same mapping as training dataset
validation_field_mapping:
  question_field: "question"
  completion_field: "completion"

# Advanced options
dataloader_num_workers: 4
remove_unused_columns: false
warmup_ratio: 0.1
weight_decay: 0.01
max_grad_norm: 1.0

# Data arguments
dataset_train_split: "train"
dataset_test_split: "test"
